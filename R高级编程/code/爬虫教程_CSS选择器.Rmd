---
title: "爬虫教程——CSS选择器"
author: "ljj"
date: "`r Sys.Date()`"
output: 
  html_document:
    highlight: espresso
    theme: united
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 5
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.show = "hold",
                      fig.align = "center",
                      tidy = FALSE)
```

## 加载经常用的R包

```{r,warning=FALSE,message=FALSE}
library(pacman)
p_load(tidyverse,stringr,DT,skimr,DataExplorer,grf,glmnet,caret,tidytext,
       explore,patchwork,ggrepel,ggcorrplot,gghighlight,ggthemes,fpp2,
       forecast,magrittr,readxl,writexl,listviewer,car,tseries,vtable)
library(RCurl)
library(rvest)
library(janitor)
library(lubridate)
```

## 爬取CRAN官网

```{r,eval=FALSE}
download.file(
  url = "https://cran.r-project.org/web/packages/available_packages_by_date.html",
  destfile = "scrapedpage.html"
)
```

```{r,eval=FALSE}
data_cran <- read_html("https://cran.r-project.org/web/packages/available_packages_by_date.html")

# data_cran %>% 
#   html_node("th , td") %>% 
#   html_table(fill = TRUE) -> data_cran_df

data_cran %>% 
  html_nodes("body > table") %>% 
  html_table(fill = TRUE) -> data_cran_df

data_cran_df %>% class()

data_cran_df %>% 
  bind_rows() %>% 
  head(100) %>% 
  datatable()

# write_csv(data_cran_df,path = "../data1/cran_20200508.csv")
```

## 爬取维基百科

```{r}
url <- "https://en.wikipedia.org/wiki/Men%27s_100_metres_world_record_progression"  # 读取URL
download.file(url, destfile = "scrapedpage.html", quiet=TRUE)  # 下载html
data_html <- read_html("scrapedpage.html")
```

```{r,eval=FALSE}
data_html %>% 
  html_nodes("div+ .wikitable td , div+ .wikitable th") %>% 
  html_table(fill = TRUE)   # 居然错了！！！
# Error in html_table.xml_node(., fill = TRUE) : html_name(x) == "table" is not TRUE
```

啊哦!似乎我们马上就遇到了一个错误。这里我就不细讲了，但是有时候我们必须对**SelectorGadget**保持谨慎。这是一个很好的工具，通常可以完美地工作。然而，有时候看起来正确的选择(例如，用黄色突出显示的东西)并不是我们想要的。我特意选择了这个Wikipedia 100m的例子，因为我想展示这个潜在的陷阱。再次强调:网络抓取既是一门科学，也是一门艺术。

```{r}
data_html %>% 
  html_nodes("#mw-content-text > div > table:nth-child(8)") %>%   # 分析网页
  html_table() -> pre_iaaf  # 得到数据表

pre_iaaf %>% class()

pre_iaaf %<>% 
  bind_rows() %>% 
  as_tibble() %>% 
  janitor::clean_names()    # 改变列名称

pre_iaaf %>% colnames()

pre_iaaf %>% datatable()
```

```{r}
pre_iaaf %<>% 
  mutate(athlete = if_else(is.na(athlete),athlete,lag(athlete)))

pre_iaaf %>% 
  mutate(date = mdy(date))-> pre_iaaf
pre_iaaf
```

最后，我们有了干净的数据框架。如果我们愿意，我们可以很容易地绘制出前国际田联的数据。但是，我打算等到我们收集完剩下的WR数据后再这么做。说到这里…

```{r}
data_html %>% 
  html_nodes("#mw-content-text > div > table:nth-child(14)") %>% 
  html_table(fill = TRUE) -> iaaf_76

iaaf_76 %>% class()

iaaf_76 %>% 
  bind_rows() %>% 
  as_tibble() %>% 
  janitor::clean_names() %>% 
  mutate(date = mdy(date)) %>% 
  mutate(athlete = ifelse(athlete=="", lag(athlete), athlete)) -> iaaf_76 
iaaf_76 %>% tail(20)

iaaf_76 %<>% 
  mutate(date = if_else(is.na(date),lag(date),date))

iaaf_76
```

```{r}
data_html %>% 
  html_nodes("#mw-content-text > div > table:nth-child(19)") %>% 
  html_table(fill = TRUE) -> iaaf

iaaf %>% 
  bind_cols() %>% 
  as_tibble() %>% 
  janitor::clean_names() %>% 
  mutate(date = mdy(date)) -> iaaf

iaaf
```

```{r}
wr100 <- 
  bind_rows(
    pre_iaaf %>% select(time, athlete, nationality:date) %>% mutate(era = "Pre-IAAF"),
    iaaf_76 %>% select(time, athlete, nationality:date) %>% mutate(era = "Pre-automatic"),
    iaaf %>% select(time, athlete, nationality:date) %>% mutate(era = "Modern")
  )
wr100
```

```{r}
wr100 %>%
  ggplot(aes(x=date, y=time, col=fct_reorder2(era, date, time))) + 
  geom_point(alpha = 0.7) +
  labs(
    title = "Men's 100m world record progression",
    x = "Date", y = "Time",
    caption = "Source: Wikipedia"
    ) +
  theme(legend.title = element_blank()) ## Switch off legend title
```

## 爬取tidyverse网站

```{r}
data_tidyverse <- read_html("https://www.tidyverse.org/blog/")
data_tidyverse %>% 
  html_nodes("#main > div.band.padForHeader.pushFooter > div > div > div.column75 > div:nth-child(2) > div:nth-child(2) > div > div.itemHeader > div.itemTitle > a") %>% 
  html_text()
```

## 爬取豆瓣

```{r}
data_douban <- read_html("https://book.douban.com/top250?icn=index-book250-all")

data_douban %>% 
  html_nodes("p.pl") %>% 
  html_text()
```

## 西域网的特种扳手品类(模板)

### 分析网页结构

http://www.ehsy.com/category-16883?p=1

http://www.ehsy.com/category-16883?p=2

```{r}
str_c("http://www.ehsy.com/category-16883?","p=",2)
```

### 读取网页

```{r}
map(1:15,function(i){
  url <- str_c("http://www.ehsy.com/category-16883?","p=",i)
  df <- tibble(url = url)
}) %>% 
  bind_rows()->url

url

data_tianyu <- read_html(url %>% pull(url))
```

### 定位代码与抽取数据

```{r}
sku <- data_tianyu %>%
  html_nodes('div.order.ell span:nth-child(2)') %>%
  html_text()

stock <-
  data_tianyu %>% html_nodes('div.proTime.ell span:nth-child(2)') %>% html_text()

price <-
  data_tianyu %>% html_nodes('div.price.ell span.yen') %>% html_text()

name <-
  data_tianyu %>% html_nodes('.p-name .high-light') %>% html_text()
```

```{r}
result <- data.frame(name,sku,stock,price)
result
```

总结：

- read_html读取html页面
- html_node结合css得到内容
- html_table得到表格内容
- as_tibble转换数据框
- clean_names得到整洁列名称
- mutate对列进行处理
- ggplot对数据进行可视化
















