--- 
title: "机器学习-数据预处理"
# subtitle: "Lecture title"
author:
- affiliation: Dongbei University of Finance & Economics
  name: Studennt. Li Junjie
date: '`r Sys.Date()`'
output:
  bookdown::html_document2:
    # code_folding: hide
    highlight: pygments
    # highlight: zenburn
    # highlight: haddock
    # theme: darkly
    theme: cerulean
    df_print: paged	
    number_sections: true
    keep_md: no
    keep_tex: no
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: no
      smooth_scroll: yes
    # css: styles.css
# bibliography: [book.bib, packages.bib]
# biblio-style: apalike
link-citations: yes
sansfont: Times New Roman
always_allow_html: yes
urlcolor: "red"
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
abstract: |
    在对数据进行分析之前，需要对数据做一些预处理，包括**数据分割**、**缺失值处理**、**删除近零方差变量**、**删除高度线性相关变量**、**数据标准化**。
---

# `r emo::ji("smile")`加载经常用的R包`r emo::ji("smile")`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE,
                      fig.show = "hold",
                      fig.align = "center",
                      tidy = FALSE)
                      # class.source = "bg-warning")
```

```{r,echo=FALSE}
library(bubblyr)
# bubblyr::bubbly(theme = "sweet")
# bubblyr::bubbly(theme = "rladies")
```

```{js, echo=FALSE}
$('.title').css('color', 'red')
$('.title').css('font-family', 'Times New Roman')
```

```{css, echo=FALSE}
* {
    # font-size: 17px !important;
    font-family: "Times New Roman" !important;
    # color: rgb(199 237	204)
}
::selection {
   # background: rgb(135 206 255);
}
```

```{r,warning=FALSE,message=FALSE}
library(pacman)
# 读数据
p_load(readxl,writexl,data.table,openxlsx,haven,rvest)
```

```{r,warning=FALSE,message=FALSE}
# 数据探索
p_load(tidyverse,DT,skimr,DataExplorer,explore,vtable,stringr,kableExtra,lubridate)
```

```{r,eval=TRUE,warning=FALSE,message=FALSE}
# 模型
p_load(grf,glmnet,caret,tidytext,fpp2,forecast,car,tseries,hdm,tidymodels,broom)
```

```{r,eval=TRUE,warning=FALSE,message=FALSE}
# 可视化
p_load(patchwork,ggrepel,ggcorrplot,gghighlight,ggthemes,shiny)
```

```{r,eval=TRUE,warning=FALSE,message=FALSE}
# 其它常用包
p_load(magrittr,listviewer,devtools,here,janitor,reticulate,jsonlite)
```

# `r emo::ji("smile")`读入数据`r emo::ji("smile")`

第一步是**读入数据**，对数据进行初步了解。下面以小说**《三生三世十里桃花》**中的人物信息为背景，具体的变量解释表如表1所示，其中**因变量Y**为“决定”这个变量。

```{r}
knitr::include_graphics(here::here("Machine_Learning_and_Causal_Inference/fig/640.png"))
```

```{r}
data <- read.csv(here::here("Machine_Learning_and_Causal_Inference/data/相亲数据2.csv"),fileEncoding = "UTF-8")
data
```

```{r}
data %>% str()
```

```{r}
data %>% glimpse()
```

```{r}
data %>% skimr::skim()
```
# `r emo::ji("smile")`分割训练集和测试集`r emo::ji("smile")`

数据拿到之后，要先划分**训练集**和**测试集**。测试集是黑盒子，是不能触碰的炸弹，所做的任何处理，包括标准化、缺失值填补都只能基于训练集。下面介绍几个典型划分训练集和测试集的方法。

## `r emo::ji("clap")`留出法`r emo::ji("clap")`

**留出法分割**是将样本分为两个互斥的子集，通常情况下，划分数据的80%为训练集，剩下的20%为测试集

之前提过，`caret包`中的`createDataPartition()函数`不仅可以实现这样的划分，而且可以保证训练集和测试集中Y的比例是一致的，简而言之就是按照Y进行**分层抽样**。

```{r}
set.seed(1234)
data_id <- createDataPartition(data$决定,p = 0.7,list = FALSE,times = 1)
data_id
```

```{r}
data_training <- data %>% slice(data_id)
data_testing <- data %>% slice(-data_id)
```

```{r}
data_training
data_testing
```
```{r}
data %$% table(决定) %>% prop.table() 
data_training %$% table(决定) %>% prop.table() 
data_testing %$% table(决定) %>% prop.table() 
```

## `r emo::ji("clap")`交叉验证法`r emo::ji("clap")`

**交叉验证法**将原始数据分成K组（一般是均分），每次训练将其中一组作为测试集，另外K-1组作为训练集。

实际应用中一般**十折交叉验证**用得最多，但是这里由于数据量太少，就以3折交叉验证为例（见图2），展示代码如下：

```{r}
set.seed(1234)
data_id1 <- createFolds(data$决定,k = 3,list = FALSE,returnTrain = TRUE)
data_id1
data_id1 %>% table()
```

```{r}
data_training <- data[-which(data_id == 1),]
data_testing <- data[which(data_id == 1),]
data_training
data_testing
```

## `r emo::ji("clap")`Bootstrap法`r emo::ji("clap")`

当数据量比较少时，**Bootstrap抽样**会成为“救命稻草”，它是一种从给定训练集中有放回的**均匀抽样**。也就是说，每当选中一个样本，它依然会被再次选中并被再次添加到训练集中。

`createResample()函数`中times参数用于设定生成几份随机样本，当**times为3**，意味着生成3份样本。不仅不同sample之间会有交叉，就连同一份sample中也会有重复的样本。

```{r}
set.seed(1234)
data_id1 <- createResample(data$决定,times = 3,list = FALSE)
data_id1
```

上面这些划分训练集和测试集的方法都是针对**横截面数据**而言的，那么对于时间序列又该如何进行数据分割呢？ 

## `r emo::ji("clap")`分割时间序列`r emo::ji("clap")`

```{r}
data_growth <- data.frame(时间 = ymd(20160101) + months(1:6),
                            数值 = 1:6)
data_growth
```

接下来，利用`caret包`来分割时间序列。`createTimeSlices()函数`需要输入以下参数：`initialWindow`表示第一个训练集中的样本数；`horizon参数`表示每个测试集中的样本数；`fixedWindow参数`表示是否每个训练集中的样本数都相同。

从结果可以看出来，**一共有2组训练集和测试集**。第一组的训练集为1、2、3、4、5行观测，测试集为6、7行观测。那么第二组呢？从下面的数据就可以看出。

```{r}
data_id <- createTimeSlices(data_growth$数值,initialWindow = 3,horizon = 2,fixedWindow = TRUE)
data_id
```

```{r}
data_id$train
data_id$test
```

```{r}
data_id1 <- createTimeSlices(data_growth$数值,initialWindow = 3,horizon = 2,fixedWindow = FALSE)
data_id1
```

# `r emo::ji("smile")`处理缺失值`r emo::ji("smile")`

`caret包`中`preProcess()函数`实现了两种常用的缺失值处理方法：**中位数填补法**、**K近邻方法**。

## `r emo::ji("clap")`中位数填补法`r emo::ji("clap")`

该方法直接用训练集的**中位数**代替缺失值，所以对于每个变量而言，填补的缺失值都相同，为训练集的中位数。该方法的优点是速度非常快，但填补的准确率有待验证。

```{r}
data <- read.csv(here::here("Machine_Learning_and_Causal_Inference/data/相亲数据2.csv"),fileEncoding = "UTF-8")
data
```

```{r}
set.seed(1234)
data_id <- createDataPartition(data$决定,p = 0.7,times = 1,list = FALSE)
data_training <- data[data_id,]
data_testing <- data[-data_id,]
data_training
data_testing
```
训练集中所有人的智力中位数值是不是7.5?

```{r}
data_training$智力 %>% median()
```

```{r}
data_training %>% datatable()
data_training %>% is.na() %>% sum()
```

```{r}
data_testing %>% datatable()
data_testing %>% is.na() %>% sum()
```

```{r}
data_imputation_median <- preProcess(data_training,method = "medianImpute")
data_imputation_median
data_training_imputation <- predict(data_imputation_median,data_training)


all.equal(data_training,data_training_imputation)  # 知道为什么是true吗？
```

```{r}
data_testing_imputation <- predict(data_imputation_median,data_testing)
data_testing_imputation
```

## `r emo::ji("clap")`K近邻法`r emo::ji("clap")`

该方法的思想是**“近朱者赤近墨者黑”**。K近邻法对于需要插值的记录，基于**欧氏距离**计算k个和它最近的观测，然后接着利用**k个近邻的数据来填补缺失值**。

**K近邻法**会自动利用训练集的**均值标准差信息**对数据进行标准化，所以最后得到的数据是标准化之后的。如果想看原始值，那么还需要将其去标准化倒推回来。

```{r}
data_imputation_knn <- preProcess(data_training,method = "knnImpute")
data_imputation_knn
data_training_imputation <- predict(data_imputation_knn,data_training)


all.equal(data_training,data_training_imputation)  # 知道为什么不是true吗？
```

```{r}
data_training$智力 <- data_training_imputation$智力 * sd(data_training$智力,na.rm = TRUE) +
  mean(data_training$智力,na.rm = TRUE)
```

```{r}
data_testing_imputation <- predict(data_imputation_knn,data_testing)
data_testing_imputation
```
```{r}
data_testing$智力 <- data_testing_imputation$智力 * sd(data_training$智力,na.rm = TRUE) + mean(data_training$智力,na.rm = TRUE)

data_testing
```

# `r emo::ji("smile")`删除近零方差`r emo::ji("smile")`

**零方差或者近零方差的变量**传递不了什么信息，因为几乎所有人的取值都一样。可以利用`caret包`中的`nearZeroVar()函数`，一行代码就能找出近零方差的变量，操作过程非常简单。

```{r}
nearZeroVar(data_training) # 4 和 10
```

```{r}
data_training_dropvariable <- data_training[,-nearZeroVar(data_training)]
data_testing_dropvariable <- data_testing[,-nearZeroVar(data_training)]
data_training_dropvariable
data_testing_dropvariable %>% str()
```

# `r emo::ji("smile")`删除共线性变量`r emo::ji("smile")`

`caret包`中的`findCorrelation()函数`会自动找到高度共线性的变量，并给出建议删除的变量。

但需要注意，这个函数对输入的数据要求比较高：

- 首先，**数据中不能有缺失值**，所以在此之前需要先处理缺失值；
- 其次，**只能包含数值型变量**。

```{r}
data_training %<>% drop_na()
data_training %>% 
  select(-nearZeroVar(data_training)) %>% 
  select(where(is.numeric)) %>% 
  cor() -> data_cor
data_cor
```

```{r}
data_high_cor <- findCorrelation(data_cor,cutoff = 0.75,verbose = TRUE,names = TRUE)

data_high_cor
```

```{r}
data_training %<>% select(-data_high_cor)
data_testing %<>% select(-data_high_cor)
data_training
data_testing
```

# `r emo::ji("smile")`标准化`r emo::ji("smile")`

为什么要**标准化？**很简单，看看年龄，几十万岁，但是智力这个变量最高也才10分，这两列变量的量纲不同，为了防止年龄的权重过高，就需要将这些特征进行标准化才能学习各个变量真实的权重。**需要注意的是**：*只能拿训练集的均值和标准差来对测试集进行标准化*。

```{r}
data_proprocess_value <- preProcess(data_training,method = c("scale","center"))
data_training_fin <- predict(data_proprocess_value,data_training)
data_testing_fin <- predict(data_proprocess_value,data_testing)
data_training
data_testing
```

