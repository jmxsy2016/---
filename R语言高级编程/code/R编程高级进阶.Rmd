---
title: "R高级编程"
author: "LJJ"
date: "2020/3/31"
output: 
  html_document:
    theme: flatly
    highlight: haddock 
    # code_folding: show
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: true
      smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=300)
## Next hook based on this SO answer: https://stackoverflow.com/a/39025054
knitr::knit_hooks$set(
  prompt = function(before, options, envir) {
    options(
      prompt = if (options$engine %in% c('sh','bash')) '$ ' else 'R> ',
      continue = if (options$engine %in% c('sh','bash')) '$ ' else '+ '
      )
    })
```

*注:这是三堂编程课中的第三堂。如果你还没听过，请看看前两节课。我们在这里所讲的内容都与之前的讲座无关。但是，我将假设您已经很好地理解了R函数和环境通常是如何工作的。我们今天的目标是通过让它们并行运行来极大地提高编程任务的速度。*

## 3.1 软件下载

### 3.1.1 R包

* New: **parallel**, **future**, **future.apply**, **furrr**, **RhpcBLASctl**, **tictoc**
* Already used: **tidyverse**, **pbapply**, **memoise**, **here**, **hrbrthemes**

下面的代码块将安装(如果需要)并为您加载所有这些包。请注意，**parallel**包与基本的R安装捆绑在一起，应该已经在您的系统上了。我还将调用`future::plan()`函数，并将解析设置为`"multiprocess"。现在不要担心这意味着什么——我将在适当的时候进行解释——只要把它看作是一种方便的方法，可以为本文档的其余部分设置所需的并行编程行为。

```{r, cache=FALSE, message=FALSE, warning=FALSE}
## Load and install the packages that we'll be using today
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tictoc, 
               parallel, 
               pbapply, 
               future, 
               future.apply, 
               tidyverse,
               hrbrthemes, 
               furrr, 
               RhpcBLASctl, 
               memoise,
               here)
## Set future::plan() resolution strategy
plan(multiprocess)
```

*注意:如果您在RStudio中运行上面的代码块，那么您将收到一条警告消息，其效果是:*

```
## Warning: [ONE-TIME WARNING] Forked processing ('multicore') is disabled
## in future (>= 1.13.0) when running R from RStudio, because it is
## considered unstable. Because of this, plan("multicore") will fall
## back to plan("sequential"), and plan("multiprocess") will fall back to
## plan("multisession") - not plan("multicore") as in the past. For more details,
## how to control forked processing or not, and how to silence this warning in
## future R sessions, see ?future::supportsMulticore
```

*再次强调，现在不要担心这个。我会在适当的时候解释的*

## 3.2 序言

前几节课，我们讨论了一些相对较新的包对r空间分析的巨大差异。以前需要同样复杂的空间操作已经被更简单、更直观的工具所取代。如果这还不够好，这些新工具也更快了。今天我们将看到一些非常相似的东西。**并行编程**是一个庞大而复杂的主题，存在许多潜在的缺陷。然而，软件创新和一些令人惊叹的新(ish)包使并行编程变得“更容易”和更安全。^[我应该强调的是，十多年来，R-core团队为并行编程提供了出色的支持。但在我看来，学习R的门槛最近已经降低了，这是毫无疑问的。] 

考虑到这一点，我将把今天的讲座从头到尾组织起来。特别地，我将以一些鼓舞人心的例子开始。我的主要目标是演示“并行”的易用性和即时回报。只有在使你相信这些事实之后，我们才能进入一些在幕后抽象出来的技术细节。本课程的后半部分将更一般地讨论并行编程(即不特定于r)，并强调您应该注意的潜在陷阱。

准备好了吗?开始学习吧。

## 3.3 例子1

我们的第一个激动人心的例子是将涉及相同的`slow_square()`函数，我们在之前的讲座中看到:

```{r slow_square}
# library(tidyverse) ## Already loaded

## Emulate slow function
slow_square <- function(x = 1){
  x_sq <- x^2
  df <- tibble(value = x,value_square = x^2)
  Sys.sleep(2)
  return(df)
}
```

让我们使用标准的`“lapply()”`方法来遍历这个函数，我们现在已经熟悉了这个方法。注意，此迭代将以*串行*方式执行。我将使用[**tictoc**](https://cran.r-project.org/web/packages/tictoc/)包来记录计时。

```{r serial_ex}
tic()
serial_ex <- lapply(1:12, slow_square) %>% bind_rows()
toc()
serial_ex
```

正如预期的那样，由于在每个连续的迭代之后(例如，每个连续的迭代)强制中断，迭代运行大约需`r parallel::detectCores()`秒。(`Sys.sleep(2)`”`)。另一方面，这意味着我们可以通过在*并行*中迭代来轻松地加快速度。

在继续之前，值得指出的是，我们并行的能力取决于我们可用的CPU内核的数量。从R中获取此信息的最简单方法是使用`parallel:: detectCores()`函数:

```{r cache=F}
# future::availableCores() ## Another option
detectCores()
```

So, I have `r parallel::detectCores()` cores to play with on my laptop.^[A Dell Precision 5530 running Arch Linux, if you're interested.] Adjust expectations for you own system accordingly.

因此，我有`r parallel::detectCores()`内核可以在我的笔记本电脑上使用。^[如 果你有兴趣，可以看看运行Arch Linux的Dell Precision 5530。]

好，回到我们的例子。我将使用[**future.apply**](https://cran.r-project.org/web/packages/future.apply/index.html)包实现并行迭代(稍后将对此进行详细介绍)。请注意，这个问题的其他参数是不变的。

```{r future_ex}
# library(future.apply)  ## Already loaded
# plan(multiprocess) ## Already set above

tic()
future_ex <- future_lapply(1:12, slow_square) %>% bind_rows()
toc()
```

哇，执行时间是`r parallel::detectCores()`倍快!更令人印象深刻的是，它的语法几乎没有变化。我基本上只需要告诉R我想要并行地实现迭代(即**plan(multiproces)**)，并稍微修改我的lapply调用(即**future_**lapply())。

我们来确认一下输出是否相同。

```{r all_equal_ex}
all_equal(serial_ex, future_ex)
```

For those of you who prefer the *`purrr::map()`* family of functions for iteration and are feeling left out; don't worry. The [**furrr**](https://davisvaughan.github.io/furrr/index.html) package has you covered. Once again, the syntax for these parallel functions will be very little changed from their serial versions. We simply have to tell R that we want to run things in parallel with `plan(multiprocess)` and then slightly amend our map call to <code>**future_**map_df**r**()</code>.^[In this particular case, the extra "r" at the end tells future to concatenate the data frames from each iteration by *rows*.]

对于那些喜欢使用`purrr::map()`函数家族进行迭代并感觉被忽略的人;别担心。[**furrr**](https://davisvaughan.github.io/furrr/index.html)可以解决你的问题你。同样，与串行版本相比，这些并行函数的语法几乎没有变化。我们只需告诉R我们想要与`plan(multiprocess)`并行运行，然后稍微修改我们对future_map_df**r**()的map调用^[在这个特殊的例子中，末尾的额外“r”告诉future将每个迭代中的数据帧按*rows*连接起来]

```{r furrr_ex}
# library(furrr)  ## Already loaded
# plan(multiprocess) ## Already set above

tic()
furrr_ex <- future_map_dfr(1:12, slow_square,.progress = TRUE)
toc()
```

这有多简单?我们几乎不需要更改原始代码，也不需要为所有这些额外的性能支付一分钱,祝贺自己已经成为**并行编程的专家**。^[我并不是在白费力气，但正如我在这门课的第一节课上所指出的:你最近看到Stata/MP许可证的价格了吗?更不用说你实际上是在为每个核心付费]

## 3.3 例子2

我们的第二个激励例子将涉及一个更现实和稍微计算密集型的案例:假设检验的自举系数值。我还将花更多的时间讨论我们正在使用的包以及它们在做什么。

首先创建一个伪数据集(our_data)并指定一个引导函数(bootstrp())。这个函数将从数据集中抽取**10,000个观察值作为样本**(带有替换)，拟合一个回归，然后提取x变量的系数。注意，考虑到我们最初是如何生成数据的，这个系数估计值应该在2左右。

```{r bootstrp}
## Set seed (for reproducibility)
set.seed(1234)
# Set sample size
n <- 1e6

## Generate a large data frame of fake data for a regression
our_data <- tibble(x = rnorm(n), 
                   e = rnorm(n)) %>%
  mutate(y = 3 + 2*x + e)

## Function that draws a sample of 10,000 observations, runs a regression and extracts
## the coefficient value on the x variable (should be around 2).
bootstrp <- function(i) {
  ## Sample the data
  sample_data <- sample_n(our_data, size = 1e4, replace = TRUE)
  ## Run the regression on our sampled data and extract the extract the x coefficient.
  x_coef <- lm(y ~ x, data = sample_data)$coef[2]
  ## Return value
  return(tibble(x_coef = x_coef))
  }
```

### 3.3.1 串行实现(用于比较)

让我们先实现串行函数，以获得比较的基准。

```{r sim_serial, dependson=bootstrp}
set.seed(123L) ## Optional to ensure that the results are the same

## 10,000-iteration simulation
tic()
sim_serial <- lapply(1:1e4, bootstrp) %>% bind_rows()
toc(log = TRUE)
```

这在我的系统上花了`r round(as.numeric(gsub(" sec elapsed", "", (tic.log()[[1]]))), 0)`秒。这不是一个巨大的痛苦，但是让我们看看如果我们切换到一个并行(多核)实现是否可以做得更好。

### 3.3.2 并行实现使用**future**生态系统

到目前为止，我们所做的所有并行编程都是建立在Henrik Bengtsson的惊人的**future包** 之上的。“future”基本上是评估代码和输出的一种非常灵活的方式。除此之外，这允许您轻松地在_serial_或_asynchronously_(即并行)评估代码之间进行切换。您只需设置您的解决方案——"sequential"、“multiprocess”、“cluster”等等——并让将来为您处理实现。

以下是亨里克[描述](https://cran.r-project.org/web/packages/future/vignettes/fu1-overview.html) 的核心思想:

> In programming, a _future_ is an abstraction for a _value_ that may be available at some point in the future. The state of a future can either be unresolved or resolved... Exactly how and when futures are resolved depends on what strategy is used to evaluate them. For instance, a future can be resolved using a sequential strategy, which means it is resolved in the current R session. Other strategies may be to resolve futures asynchronously, for instance, by evaluating expressions in parallel on the current machine or concurrently on a compute cluster

正如我试图强调的那样，**future**是相对较新的景象。这当然不是在r中实现并行过程的第一种方法，也不是唯一的方法。但是，我认为它提供了一个简单而统一的框架，这使它成为了首选。更重要的是，我们在这里使用的相同命令将非常方便地用于涉及高性能计算集群的更复杂的设置。我们将在本课程的大数据部分亲身体验这一点。

You've probably also noted that keep referring to the "future ecosystem". This is because **future** provides the framework for other packages to implement parallel versions of their functions. The two that I am focusing on today are

你可能也注意到，不断提到“future ecosystem”。 这是因为**future**为其他包提供了实现其函数的并行版本的框架。今天我主要讲两个方面:

1. the [**future.apply**](https://cran.r-project.org/web/packages/future.apply/index.html) package (also by Henrik), and
2. the [**furrr**](https://davisvaughan.github.io/furrr/index.html) package (an implementation for **purrr** by [Davis Vaughan](https://twitter.com/dvaughan32)).

在这两种情况下，我们都从制定解决future评估的计划开始。我建议使用“`plan(multiprocess)`”，这是一种方便的方法，可以告诉包为我们的特定系统选择最优的并行策略。然后我们调用我们的函数——这涉及到对它们的串行等价物的微小修改——并让未来的魔法来处理其他的事情。

#### 3.3.2.1 future.apply

这是`future.apply :: future_lapply（）`并行实现。 注意，我添加`future.seed = 123L`选项只是为了确保结果相同。 但是，这不是必需的。

```{r sim_future, dependson=bootstrp}
# library(future.apply)  ## Already loaded
# plan(multiprocess) ## Already set above

## 10,000-iteration simulation
tic()
sim_future <- future_lapply(1:1e4, bootstrp, future.seed=123L) %>% bind_rows()
toc()
```

#### 3.3.2.2 furrr

这是`furrr :: future_map_dfr（）`的实现。 与上述类似，请注意，我只是添加`.options = future_options（seed = 123L）`选项以确保输出完全相同。

```{r sim_furrr, dependson=bootstrp}
# library(furrr)  ## Already loaded
# plan(multiprocess) ## Already set above

## 10,000-iteration simulation
tic()
sim_furrr <- future_map_dfr(1:1e4, bootstrp, 
                            .progress = TRUE,
                            .options=future_options(seed=123L))
toc()
```

### 3.3.3 Results

不出所料，我们通过并行处理大大减少了总的计算时间。 但是请注意，此示例的并行改进并未随我系统上内核的数量线性增加（即r parallel:: detectCores（））。 原因与运行并行实现的“开销”有关—我将在本文档底部更深入地讨论这个主题。

虽然这并非完全是艰苦的工作，但我认为我们应该以漂亮的情节形式看到自举练习的结果。 我将为此使用sim_furrr结果数据框，尽管因为它们都是相同的，但这并不重要。 如您所见，估计的系数值紧密围绕我们的模拟平均值2聚集


```{r x_coef, dependson=sim_furrr}
sim_furrr %>%
  ggplot(aes(x_coef)) +
  geom_density(col=NA, fill="gray25", alpha=0.3) +
  geom_vline(xintercept=2, col="red") +
  labs(
    title = "Bootstrapping example",
    x="Coefficient values", y="Density",
    caption = "Notes: Density based on 10,000 draws with sample size of 10,000 each."
    )
```


### 3.3.4 Other parallel options

Futures并不是R中进行并行编程的唯一方法。我要简要提及的另一个选择是**pbapply软件包**。正如我们在第一次编程讲座中看到的那样，该程序包在*apply函数上提供了一个轻量级包装，该包装增加了进度条。但是，该软件包还为多核实现添加了一个非常方便的选项。您基本上只需将cl = CORES添加到call中。尽管它不依赖于futures，但pbapply还可以为您解决所有特定于操作系统的开销。 

注意:您将需要交互式地运行下一个块来查看进度条。顺便说一句，furrr也支持进度条。

```{r sim_pblapply}
set.seed(123) ## Optional to ensure results are exactly the same.

# library(pbapply) ## Already loaded

## 10,000-iteration simulation
tic()
sim_pblapply <- pblapply(1:1e4,
                         bootstrp, 
                         cl = parallel::detectCores()) %>%
  bind_rows()
toc()
```

## 3.4 一般并行编程主题

Motivating examples out of the way, let's take a look underneath the hood. I want to emphasise that this section is **more "good to know" than "need to know"**. Even if you take nothing else away from rest of this lecture, you are already well placed to begin implementing parallel functions at a much larger scale.

然而,……虽然为了在R中并行编程，您不*“需要”*了解下一节，但是扎实地掌握基础知识是有价值的。它将使您更好地理解并行编程的一般工作方式，并帮助您了解**future**和co.在幕后为您做了多少工作。它还将帮助您理解为什么相同的代码在某些系统上运行得比其他系统快，并避免一些常见的陷阱。

### 3.4.1 术语

I'll start by clearing up some terminology.

- **Socket:** The physical connection on your computer that houses the processor. Most work and home computers --- even very high-end ones --- only have one socket and, thus, one processor. However, they can have multiple cores. Speaking of which...
- **Core:** The part of the processor that actually performs the computation. Back in the day, processors were limited to a single core. However, most modern processors now house multiple cores. Each of these cores can perform entirely separate and independent computational processes.
- **Process:** A single instance of a running task or program (R, Dropbox, etc). A single core can only run one process at a time. However, it may give the appearance of doing more than that by efficiently scheduling between them. Speaking of which...
- **Thread:** A component or subset of a process that can, inter alia, share memory and resources with other threads. We'll return to this idea as it applies to *hyperthreading* in a few paragraphs.
- **Cluster:** A collection of objects that are capable of hosting cores. This could range from a single socket (on your home computer) to an array of servers (on a high-performance computing network).

You may wondering where the much-referenced **CPU** (i.e. central processing unit) fits into all of this. Truth be told, the meaning of CPU has evolved with the advent of new technology like multicore processors. For the purposes of this lecture I will use the following definition: 

$$\text{No. of CPUs} = \text{No. of sockets} \times \text{No. of physcial cores} \times \text{No. of threads per core}$$

If nothing else, this is consistent with the way that my Linux system records information about CPU architecure via the [lscpu](https://linux.die.net/man/1/lscpu) shell command: 

```{bash lscpu, error=T, prompt=T}
## Only works on Linux
lscpu | grep -E '^Thread|^Core|^Socket|^CPU\('
```

Note that the headline "CPU(s)" number is the same that I got from running `parallel::detectCores()` earlier (i.e. `r parallel::detectCores()`). 

### 3.4.2 A bit more about logical cores and hyperthreading

Logical cores extend or emulate the ability of physical cores to perform additional tasks. The most famous example is Intel's [**hyperthreading**](https://en.wikipedia.org/wiki/Hyper-threading) technology, which allows a single core to switch very rapidly between two different tasks. This mimics the appearance and performance (albeit to a lesser extent) of an extra physical core. You may find [this YouTube video](https://www.youtube.com/watch?v=mSZpDF-zUoI&) helpful for understanding the difference in more depth, including a nice analogy involving airport security lines.

Taking a step back, you don't have to worry too much about the difference between physical and logical (hyperthreaded) cores for the purpose of this lecture. R doesn't care whether you run a function on a physical core or a logical one. Both will work equally well. (Okay, the latter will be a little slower.) Still, if you are interested in determining the number of physical cores versus logical cores on your system, then there are several ways to this from R. For example, you can use the [RhpcBLASctl package](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html).

```{r cores_vs_procs}
# library(RhpcBLASctl) ## Already loaded

get_num_procs() ## No. of all cores (including logical/hyperthreaded)
get_num_cores() ## No. of physical cores only
```

### 3.4.3 Forking vs Sockets

As I keep saying, it's now incredibly easy to run parallel programs in R. The truth is that it has actually been easy to do so for a long time... but the implementation used to vary by operating system. In particular, simple parallel implementations that worked perfectly well on Linux or Mac didn't work on Windows (which required a lot more overhead). For example, take a look at the [help documentation](https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/mclapply.html) for the `parallel::mclapply()` function, which has been around since 2011. If you did so, you would see a warning that `mclapply()` *"relies on forking and hence is not available on Windows"*.

Now, we clearly didn't encounter any OS-specific problems when we ran the parallel versions of our motivating examples above. The same code worked for everyone, including anyone using Windows. ~~*Loud booing.*~~ What was happening behind the scenes is that the **future** (and **pbapply**) packages automatically handled any complications for us. The parallel functions were being executed in a way that was optimised for each person's OS.

But what is "forking" and why does it matter what OS I am using anyway? Those are good questions that relate to the method of parallelization (i.e. type of cluster) that your system supports. The short version is that there are basically two ways that code can be parallelized:

- **Forking** works by cloning your entire R environment to each separate core. This includes your data, loaded packages, functions, and any other objects in your current session. This is very efficient because you don't have to worry about reproducing your "master" environment in each "worker" node. Everything is already linked, which means that you aren't duplicating objects in memory. However, forking is not supported on Windows and can also cause problems in a GUI or IDE like RStudio. 
- **Parallel sockets** (aka "PSOCKs") work by launching a new R session in each core. This means that your master environment has to be copied over and instantiated separately in each parallel node. This requires greater overhead and causes everything to run slower, since objects will be duplicated across each core. Technically, a PSOCK works by establishing a network (e.g. as if you were connected to a remote cluster), but everything is self-contained on your computer. This approach can be implemented on every system, including Windows. 

I've summarised the differences between the two approaches in the table below. The general rule of thumb is that you should use forking if it is available to you. And, indeed, this is exactly the heuristic that the future ecosystem follows via the `plan(multiprocess)` function.

| Forking        | Parallel socket         |
| ------------- |-------------|
| ✓ Faster and more memory efficient than sockets. |  ×	Slower and more memory-intensive than forking. |
| ✓️ Trivial to implement. | × Harder to implement. |
| ×	Only available for Unix-based systems like Linux and Mac (not Windows). | ✓ Works on every operating system (including Windows). |
| ×	Can cause problems when running through a GUI or IDE like RStudio.^[The reason is that shared GUI elements are being shared across child processes. (See the "GUI/embedded environments" section [here](https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/mcfork.html).) To be fair, I've only ever run into a problem once or twice while running a forking process through RStudio. These have invariably involved very time-consuming functions that contain a bunch of nested while-loops. (I suspect the different worker processes began to move out of sync with one another.) However, I want you to be aware of it, so that you aren't caught by surprise if it ever happens to you. If it does, then the solution is simply to run your R script from the terminal using, say `$ Rscript myscript.R`.] | ✓ ️No risk of cross-contamination, since each process is run as a unique node. |


## 3.5 Explicit vs implicit parallelization

Thus far we have only been concerned with *explicit* parallelization. As in, we explicitly told R to run a particular set of commands in parallel. But there is another form of *implicit* parallelization that is equally important to be aware of. In this case, certain low-level functions and operations are automatically run in parallel regardless of whether we told R to do so or not. Implicit parallelization can make a big difference to performance, but is not the default behaviour in R. So you have to enable it first. Moreover, combining explicit and implicit parallelization can cause problems if you don't take certain precautions. Let's take a look at where implicit parallelization enters the fray.

### 3.5.1 BLAS/LAPACK

Did you ever wonder how R and other programming languages perform their calculations? For example, how does R actually do things like vector addition, or scalar and matrix multiplication? The answer is [**BLAS**](http://www.netlib.org/blas/) (**B**asic **L**inear **A**lgebra **S**uprograms). BLAS are a collection of low-level routines that provide standard building blocks for performing basic vector and matrix operations. These routines are then incoporated in related libraries like [**LAPACK**](http://www.netlib.org/lapack/) (**L**inear **A**lgebra **Pack**age), which provide their own routines for solving systems of linear equations and linear least squares, calculating eigenvalues, etc. In other words, BLAS and LAPACK provide the linear algebra framework that supports virtually all of statistical and computational programming 

R ships with its own BLAS/LAPACK libraries by default. These libraries place a premium on stablility (e.g. common user experience across operating systems). While the default works well enough, you can get *significant* speedups by switching to more optimized libraries such as the [Intel Math Kernel Library (MKL)](https://software.intel.com/en-us/mkl) or [OpenBLAS](https://www.openblas.net/). Among other things, these optimised BLAS libraries support multi-threading. So now you are using all your available computer power to, say, solve a matrix.

You can use the `sessionInfo()` command to see which BLAS/LAPACK library you are using. For example, I am using OpenBLAS on this computer:

```{r blas_info}
sessionInfo()[c("BLAS", "LAPACK")]
```

### 3.5.2 Beware resource competition

While this all sounds great --- and I certainly recommend taking a look at MKL or OpenBLAS --- there is a potential downside. In particular, you risk competing with yourself for computational resources (i.e. memory) if you mix explicit and implicit parallel calls. For instance, if you run explicit multicore functions from within R on a system that has been configured with an optimised BLAS. As [Dirk Eddelbuettel](http://dirk.eddelbuettel.com/) succintly puts it in [this Stack Overflow thread](https://stackoverflow.com/a/18291826):

> There is one situation you want to avoid: (1) spreading a task over all _N_ cores and (2) having each core work on the task using something like OpenBLAS or MKL with all cores. Because now you have an _N_ by _N_ contention: each of the N task wants to farm its linear algebra work out to all _N_ cores.

Now, I want to emphasise that this conflict rarely matters in my own experience. I use optimised BLAS libraries and run explicit parallel calls all the time in my R scripts. Despite this, I have hardly ever run into a problem. Moreover, when these slowdowns have occured, I've found the effect to be relatively modest.^[The major cost appears to be the unnecessary duplication of objects in memory.] Still, I have read of cases where the effect can be quite dramatic (e.g. [here](https://stat.ethz.ch/pipermail/r-sig-hpc/2014-February/001846.html)) and so I wanted you to be aware of it all the same.

Luckily, there's also an easy and relatively costless solution: Simply turn off BLAS multi-threading. It turns out this has a negligible impact on performance, since most of the gains from optimised BLAS are actually coming from improved math vectorisation, not multi-threading. (See [this post](https://blog.revolutionanalytics.com/2015/10/edge-cases-in-using-the-intel-mkl-and-parallel-programming.html) for a detailed discussion.) You can turn off BLAS multi-threading for the current R session via the `RhpcBLASctl::blas_set_num_threads()` function. For example, I sometimes include the following line at the top of an R script:
```{r eval=F}
# blas_get_num_procs() ## If you want to find the existing number of BLAS threads
RhpcBLASctl::blas_set_num_threads(1) ## Set BLAS threads to 1 (i.e. turn off multithreading)
```

Since this is only in effect for the current R session, BLAS multithreading will be restored when I restart R.^[I could also reinstate the original behaviour in the same session by running `blas_set_num_threads(parallel::detectCores())`. You can turn off multithreading as the default mode by altering the configuration file when you first build/install your preferred BLAS library. However, that's both complicated and unecessarily restrictive in my view.] 

## 3.6 Miscellaneous

### 3.6.1 When should I go parallel?

The short answer is that you want to invoke the multicore option whenever you are faced with a so-called "[embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel)" problem. You can click on that link for a longer description, but the key idea is that these computational problems are easy to break up into smaller chunks. You likely have such a case if the potential code chunks are independent and do not need to communicate in any way. Classic examples include bootstrapping (since each regression or resampling iteration is drawn independently) and Markov chain Monte Carlo (i.e. [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)).

Having said that, there are limitations to the gains that can be had from parallelization. Most obviously, there is the [computational overhead](https://en.wikipedia.org/wiki/Overhead_(computing)) associated with splitting up the problem, tracking the individual nodes, and then bringing everything back into a single result. This can be regarded as an issue largely affecting shorter and smaller computations. In other words, the overhead component of the problem tends to diminish in relative size as the overall computation time increases. 

On the opposite end of the spectrum, there is [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law) (generalised as [Gustafson's law](https://en.wikipedia.org/wiki/Gustafson%27s_law)). This formalises the intuitive idea that there are diminishing returns to parallelization, depending on the proportion of your code that can be run in parallel. A case in point is Bayesian MCMC routines, which typically include a fixed "burn-in" period regardless of how many parallel chains are being run in parallel.

```{r amdahl, echo=F}
expand.grid(num_cores=1:2048, par_prop=c(0.5,0.75,0.9,0.95)) %>%
  mutate(
    speed_up = 1/(1 - par_prop + par_prop/num_cores),
    par_prop = scales::percent(par_prop, accuracy=1)
    ) %>%
  ggplot(aes(x = num_cores, y = speed_up, col = par_prop, group = par_prop)) +
  geom_line() +
  scale_x_continuous(trans = scales::log_trans(2), breaks = 2^c(0:11)) +
  scale_color_brewer(name = "Parallel proportion", palette = "Set1") +
  labs(
    title = "Amdahl's law",
    x = "No. of cores", y = "Theoretical speedup"
    ) +
  guides(colour = guide_legend(reverse=T))
```

### 3.6.2 How many cores should I use?

If you look this question up online, you'll find that most people recommend using `detectCores()-1`. This advice stems from the idea that you probably want to reserve one core for other tasks, such as running your web browser or word processor. While I don't disagree, I typically use all available cores for my parallel computations. For one thing, I do most of my heavy computational work in the cloud (i.e. on a server or virtual machine). So keeping some computational power in reserve doesn't make sense. Second, when I am working locally, I've gotten into the habit of closing all other applications while a parallel function is running. Your mileage may vary, though. (And remember the possible diminishing returns brought on by Amdahl's law).  FWIW, calling `plan(multiprocess)` automatically defaults to using all cores. You can change that by running, say, `plan(multiprocess(workers=detectCores()-1))`.

### 3.6.3 Fault tolerance (error catching, caching, etc.)

In my experience, the worst thing about parallel computation is that it is very sensitive to failure in any one of its nodes. An especially frustrating example is the tendency of parallel functions to ignore/hide critical errors up until the very end when they are supposed to return output. ("Oh, so you encountered a critical error several hours ago, but just decided to continue for fun anyway? Thanks!") Luckily, all of the defensive programming tools that we practiced in the [previous lecture](https://raw.githack.com/uo-ec607/lectures/master/11-funcs-adv/11-funcs-adv.html) --- catching user errors and caching intermediate results --- carry over perfectly to their parallel equivalents. Just make sure that you use a persistent cache.

**Challenge:** Prove this to yourself by running a parallel version of the cached iteration that we practiced last time. Specifically, you should [recreate](https://raw.githack.com/uo-ec607/lectures/master/11-funcs-adv/11-funcs-adv.html#aside_1:_caching_across_r_sessions) the `mem_square_verbose()` function, which in turn relies on the `mem_square_persistent()` function.^[To clarify: The verbose option simply provides helpful real-time feedback to us. However, the underlying persistent cache location --- provided in this case by `mem_square_persistent()` --- is necessary whenever you want to use a memoised function in the futures framework.] You should then be able to run `future_map_dfr(1:10, mem_square_verbose)` and it will automatically return the previously cached results. After that, try `future_map_dfr(1:24, mem_square_verbose)` and see what happens.

### 3.6.4 Random number generation

Random number generation (RNG) can become problematic in parallel computations (whether trying to ensure the same of different RNG across processes). R has various safeguards against this and future [automatically handles](https://www.jottr.org/2017/02/19/future-rng/) RNG via the `future.seed` argument. We saw an explicit example of this in example 2 [above](#1)_futureapply).

### 3.6.5 Parallel regression

A number of regression packages in R are optimised to run in parallel. For example, the superb [**fixest**](https://github.com/lrberge/fixest/wiki) and [**lfe**](https://cran.r-project.org/web/packages/lfe/index.html) packages that we saw in the lecture on regression analysis will automatically invoke multicore capabilities when fitting high dimensional fixed effects models. The many Bayesian packages in R are also all capable of --- and, indeed, expected to --- fit regression models by running their MCMC chains in parallel (e.g. [**rStan**](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#running-multiple-chains-in-parallel)). Finally, you may be interested in the [**partools**](https://cran.r-project.org/web/packages/partools/index.html) package, which provides convenient aliases for running a variety of statistical models and algorithms in parallel.

### 3.6.6 CPUs vs GPUs

Graphical Processing Units, or GPUs, are specialised chipsets that were originaly built to perform the heavy lifting associated with rendering graphics. It's important to realise that not all computers have GPUs. Most laptops come with so-called [integrated graphics](https://www.laptopmag.com/articles/intel-hd-graphics-comparison), which basically means that the same processor is performing both regular and graphic-rendering tasks. However, gaming and other high-end laptops (and many desktop computers) include a dedicated GPU card. For example, the Dell Precision 5530 that I'm writing these lecture notes on has a [hybrid graphics](https://wiki.archlinux.org/index.php/hybrid_graphics) setup with two cards: 1) an integrated Intel GPU (UHD 630) and 2) a discrete NVIDIA Quadro P2000.

So why am I telling you this? Well, it turns out that GPUs also excel at non-graphic computation tasks. The same processing power needed to perform the millions of parallel calculations for rendering 3-D games or architectural software, can be put to use on scientific problems. How exactly this was discovered involves an interesting backstory of supercomputers being built with Playstations. (Google it.) But the short version is that modern GPUs comprise *thousands* of cores that can be run in parallel. Or, as my colleague [David Evans](http://econevans.com/) once memorably described it to me: "GPUs are basically just really, really good at doing linear algebra."

Still, that's about as much as I want to say about GPUs for now. Installing and maintaining a working GPU setup for scientific purposes is a much more complex task. (And, frankly, overkill for the vast majority of econometric or data science needs.) We may revisit the topic when we get to the machine learning section of the course in a few weeks.^[Advanced machine learning techniques like [deep learning](https://blog.rstudio.com/2018/09/12/getting-started-with-deep-learning-in-r/) are particularly performance-dependent on GPUs.] Thus, and while the general concepts carry over, everything that we've covered today is limited to CPUs.

### 3.6.7 Monitoring multicore performance

Bash-compatible shells should come with the built-in `top` command, which provides a real-time view of running processes and resource consumption. (Pro-tip: Hit "1" to view processes across individual cores and "q" to quit.) An enhanced alternative that I really like and use all the time is [**htop**](https://hisham.hm/htop/), which is available on both Linux and Mac. (Windows users can install `htop` on the WSL that we covered way back in the [shell lecture](https://raw.githack.com/uo-ec607/lectures/master/03-shell/03-shell.html#windows).). It's entirely up to you whether you want to install it. Your operating system almost certainly provides built-in tools for monitoring processes and resource useage (e.g. [System Monitor](https://wiki.gnome.org/Apps/SystemMonitor)). However, I wanted to flag `htop` before we get to the big data section of the course. We'll all be connecting to remote Linux servers at that point and a shell-based (i.e. non-GUI) process monitor will prove very handy for tracking resource use.

## 3.7 Further resources

- Dirk Eddelbuettel provides a comprehensive overview of all things R parallel in his new working paper, [*Parallel Computing With R: A Brief Review*](https://arxiv.org/abs/1912.11144). I'm confident that this will become the authoritative reference once it is published.
- Beyond Dirk's article, I'd argue that the starting point for further reading should be the future package vignettes ([one](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html), [two](https://cran.r-project.org/web/packages/future/vignettes/future-2-output.html), [three](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html), [four](https://cran.r-project.org/web/packages/future/vignettes/future-4-issues.html), [five](https://cran.r-project.org/web/packages/future/vignettes/future-5-startup.html)). There's a lot in there, so feel free to pick and choose.
- Similarly, the [furrr package vignette](https://davisvaughan.github.io/furrr/index.html) is very informative (and concise).
- The [parallel package vignette](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf) provides a very good overview, not only its own purpose, but of parallel programming in general. Particular attention is paid to the steps needed to ensure a stable R environment (e.g. across operating systems).
- Finally, there a number of resources online that detail older parallel programming methods in R (`foreach`, `mclapply`, `parLapply` `snow`, etc.). While these methods have clearly been superseded by the future package ecosystem in my mind, there is still a lot of valuable information to be gleaned from understanding them. Two of my favourite resources in this regard are: [How-to go parallel in R](http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/) (Max Gordon) and [Beyond Single-Core R](https://github.com/ljdursi/beyond-single-core-R) (Jonathan Dursi).
